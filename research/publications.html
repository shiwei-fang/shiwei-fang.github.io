<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-611R9WFK9T"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-611R9WFK9T');
    </script>

    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Shiwei Fang - Research</title>
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/custom.css" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>
    <nav class="navbar navbar-expand-sm navbar-dark bg-dark sticky-top">
        <div class="container">
            <a class="navbar-brand" href="../index.html"><b>&nbsp;&nbsp;Shiwei Fang</b></a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown"
                aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse justify-content-between" id="navbarNavDropdown">
                <ul class="nav nav-pills navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html">&nbsp;Home&nbsp;</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="../Shiwei_Fang_CV.pdf" target="_blank">&nbsp;CV&nbsp;</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" href="publications.html">&nbsp;Publications&nbsp;</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="../projects/projects.html">&nbsp;Projects&nbsp;</a>
                    </li>
                </ul>
                </form>
            </div>
        </div>
    </nav>



    <div class="row  justify-content-center">
        <div class="col"></div>

        <div class="col-xl-7" style="max-width: 80rem;">
            <div class="container-fluid"
                style="background-color: transparent; padding-left: 1rem;padding-right: 1rem; padding-top: 1.5rem; padding-bottom: 2rem; min-height: 100%">

                <!-- <div class="jumbotron" style="background-color: transparent; color: #595959; padding-left: 1rem;padding-right: 1rem; padding-top: 1.5rem; padding-bottom: 2rem; min-height: 100%"> -->

                    <h4 style="text-align: left; padding-left: 5px;"><b>Publications:</b></h4>

                    <hr style="height: 0.125rem; background: grey;">

                    <h3 style="text-align: left; padding-left: 10px; font-size: 1.3rem; padding-bottom: 10px;"><b>Journal & Conference:</b></h3>

                    

                    <div class="container-pub">

                        <h6><b>IoBT-MAX: a Multimodal Analytics eXperimentation Testbed for IoBT Research</b></h6>
                        <p>Benjamin M Marlin, Niranjan Suri, <b>Shiwei Fang</b>, Mani B Srivastiva, Colin Samplawski, Ziqi Wang, Maggie Wigness</p>
                        <p>Military Communication Conference (MILCOM). IEEE, Oct. 2023.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#iobtmax" aria-expanded="false" aria-controls="iobtmax">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1109/MILCOM58377.2023.10356347" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="iobtmax">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        This paper describes the development and implementation of IoBT-MAX, a multimodal analytics experimentation testbed designed to support research and evaluation of Internet of Battlefield Things (IoBT) technologies. The testbed consists of a distributed set of edge nodes with multimodal sensing and compute capabilities coupled with a high-precision GPS localization system, and a remote monitoring and control platform. The testbed is designed to support research on multiple analytic tasks including object classification, object detection, multi-object tracking, data compression, and communication efficient inference and scheduling. The testbed has been deployed at the Robotics Research Collaboration Campus (R2C2), a DEVCOM Army Research Laboratory (ARL) facility, and is a key research instrumentation project of ARL’s Internet of Battlefield Things Collaborative Research Alliance. 
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br/>

                        <h6><b>CarFi: Rider Localization Using Wi-Fi CSI</b></h6>
                        <p>Sirajum Munir*, Hongkai Chen*, <b>Shiwei Fang</b>*, Mahathir Monjur, Shan Lin, Shahriar Nirjon</p>
                        <p>(* Equal Contribution)</p>
                        <p>2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart Systems (MASS), IEEE, Sept. 2023</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#carfimass" aria-expanded="false" aria-controls="carfimass">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1109/MASS58611.2023.00072" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="carfimass">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        With the rise of hailing services, people are increasingly relying on shared mobility (e.g., Uber, Lyft) drivers to pick up for transportation. However, such drivers and riders have difficulties finding each other in urban areas as GPS signals get blocked by skyscrapers, in crowded environments (e.g., in stadiums, airports, and bars), at night, and in bad weather. It wastes their time, creates a bad user experience, and causes more CO2 emissions due to idle driving. In this work, we explore the potential of Wi-Fi to help drivers to determine the street side of the riders. Our proposed system is called CarFi that uses Wi-Fi CSI from two antennas placed inside a moving vehicle and a data-driven technique to determine the street side of the rider. By collecting real-world data in realistic and challenging settings by blocking the signal with other people and other parked cars, we see that CarFi is 95.44% accurate in rider-side determination in both line of sight (LoS) and non-line of sight (nLoS) conditions, and can be run on an embedded GPU in real-time. 
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br/>

                        <h6><b>Heteroskedastic Geospatial Tracking with Distributed Camera Networks</b></h6>
                        <p>Colin Samplawski, <b>Shiwei Fang</b>, Ziqi Wang, Deepak Ganesan, Mani Srivastava, Benjamin M. Marlin</p>
                        <p>Proceedings of the 39th Conference on Uncertainty in Artificial Intelligence (UAI 2023). July. 2023.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#UAI23Abs" aria-expanded="false" aria-controls="UAI23Abs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://proceedings.mlr.press/v216/samplawski23a/samplawski23a.pdf" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="UAI23Abs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        Visual object tracking has seen significant progress in recent years. However, the vast majority of this work focuses on tracking objects within the image plane of a single camera and ignores the uncertainty associated with predicted object locations. In this work, we focus on the geospatial object tracking problem using data from a distributed camera network. The goal is to predict an object’s track in geospatial coordinates along with uncertainty over the object’s location while respecting communication constraints that prohibit centralizing raw image data. We present a novel single-object geospatial tracking data set that includes high-accuracy ground truth object locations and video data from a network of four cameras. We present a modeling framework for addressing this task including a novel backbone model and explore how uncertainty calibration and fine-tuning through a differentiable tracker affect performance. 
                                    </div>
                                    <!-- <a class="btn btn-proj btn-lg" href="">Project Page</a> -->
                                </div>
                            </div>
                        </div>

                        <br />


                        <h6><b>Optimizing Intelligent Edge-clouds with Partitioning, Compression and Speculative Inference</b></h6>
                        <p><b>Shiwei Fang</b>, Jin Huang, Colin Samplawski, Deepak Ganesan, Benjamin Marlin, Tarek Abdelzaher, and Maggie B. Wigness</p>
                        <p>Military Communication Conference (MILCOM). IEEE, DEC. 2021.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#MilcomAbs" aria-expanded="false" aria-controls="MilcomAbs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1109/MILCOM52596.2021.9653126" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="MilcomAbs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        Internet of Battlefield Things (IoBTs) are well positioned to take advantage of recent technology trends that have led to the development of low-power neural accelerators and low-cost high-performance sensors. However, a key challenge that needs to be dealt with is that despite all the advancements, edge devices remain resource-constrained, thus prohibiting complex deep neural networks from deploying and deriving actionable insights from various sensors. Furthermore, deploying sophisticated sensors in a distributed manner to improve decision-making also poses an extra challenge of coordinating and exchanging data between the nodes and server. We propose an architecture that abstracts away these thorny deployment considerations from an end-user (such as a commander or warfighter). Our architecture can automatically compile and deploy the inference model into a set of distributed nodes and server while taking into consideration of the resource availability, variation, and uncertainties. 
                                    </div>
                                    <!-- <a class="btn btn-proj btn-lg" href="">Project Page</a> -->
                                </div>
                            </div>
                        </div>

                        <br />


                        <h6><b>Exploiting Scene and Body Contexts in Controlling Continuous Vision Body Cameras</b></h6>
                        <p><b>Shiwei Fang</b>, Ketan Mayer-Patel, Shahriar Nirjon</p>
                        <p>Ad Hoc Networks Journal, Elsevier, Volumn 113, Mar. 2021.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#ZenCamJournalAbs" aria-expanded="false" aria-controls="ZenCamJournalAbs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1016/j.adhoc.2020.102373" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="ZenCamJournalAbs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        Ever-increasing performance at decreasing price has fueled camera deployments in a wide variety of real-world applications—making the case stronger for battery-powered, continuous-vision camera systems. However, given the state-of-the-art battery technology and embedded systems, most battery-powered mobile devices still do not support continuous vision. In order to reduce energy and storage requirements, there have been proposals to offload energy-demanding computations to the cloud Naderiparizi et al. (2016), to discard uninteresting video frames Naderiparizi et al. (2017), and to use additional sensors to detect and predict when to turn on the camera Bahl et al. (2012) . However, these proposals either require a fat communication bandwidth or have to sacrifice capturing of important events.

                                        <br />

                                        In this paper, we present — ZenCam, which is an always-on body camera that exploits readily available information in the encoded video stream from the on-chip firmware to classify the dynamics of the scene. This scene-context is further combined with simple inertial measurement unit (IMU)-based activity level-context of the wearer to optimally control the camera configuration at run-time to keep the device under the desired energy budget. We describe the design and implementation of ZenCam and thoroughly evaluate its performance in real-world scenarios. Our evaluation shows a 29.8%–35% reduction in energy consumption and 48.1-49.5% reduction in storage usage when compared to a standard baseline setting of 1920x1080 at 30fps while maintaining a competitive or better video quality at the minimal computational overhead.
                                    </div>
                                    <!-- <a class="btn btn-proj btn-lg" href="">Project Page</a> -->
                                </div>
                            </div>
                        </div>

                        <br />
                        
                        <h6><b>EyeFi: Fast Human Identification Through Vision and WiFi-based Trajectory Matching</b></h6>
                        <p><b>Shiwei Fang</b>, Md Tamzeed Islam, Sirajum Munir, Shahriar Nirjon</p>
                        <p>International Conference on Distributed Computing in Sensor Systems (DCOSS). IEEE, May 2020.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#eyefiAbs" aria-expanded="false" aria-controls="eyefiAbs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1109/DCOSS49796.2020.00022" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="eyefiAbs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        Human sensing, motion trajectory estimation, and identification are central to a wide range of applications in many domains such as retail stores, surveillance, public safety, public address, smart homes and cities, and access control. Existing solutions either require facial recognition or installation and maintenance of multiple units, or they lack long-term re-identification capability. In this paper, we propose a novel system -- called EyeFi -- that combines WiFi and camera on a standalone device to overcome these limitations. EyeFi integrates a WiFi chipset to an overhead camera and fuses motion trajectories obtained from both vision and RF modalities to identify individuals. In order to do that, EyeFi uses a student-teacher model to train a neural network to estimate the Angle of Arrival (AoA) of WiFi packets from the CSI values. Based on extensive evaluation using real-world data, we observe that EyeFi improves WiFi CSI based AoA estimation accuracy by more than 30% and offers 3,800 times computational speed over the state-of-the-art solution. In a real-world environment, EyeFi's accuracy of person identification averages 75% when the number of people varies from 2 to 10.
                                    </div>
                                    <a class="btn btn-proj btn-lg" href="../projects/eyefi.html">Project Page</a>
                                </div>
                            </div>
                        </div>

                       
                        <br />

                        <h6><b>SuperRF: Enhanced 3D RF Representation Using Stationary Low-Cost mmWave Radar</b></h6>
                        <p><b>Shiwei Fang</b>, Shahriar Nirjon</p>
                        <p>International Conference on Embedded Wireless Systems and Networks (EWSN). ACM, Feb. 2020.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#superrfAbs" aria-expanded="false" aria-controls="superrfAbs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="http://www.ewsn.org/file-repository/ewsn2020/120_131_fang.pdf" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="superrfAbs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        This paper introduces SuperRF – which takes radio frequency (RF) signals from an off-the-shelf, low-cost, 77GHz mmWave radar and produces an enhanced 3D RF representation of a scene. SuperRF is useful in scenarios where camera and other types of sensors do not work, or not allowed due  to privacy concerns, or their performance is impacted due to bad lighting conditions and occlusions, or an alternate RF sensing system like synthetic aperture radar (SAR) is too large, inconvenient, and costly. Applications of SuperRF includes navigation and planning of autonomous and semi-autonomous systems, human-robot interactions and social robotics, and elderly and/or patient monitoring in-home healthcare scenarios. We use low-cost, off-the-shelf parts to capture RF signals and to train SuperRF. The novelty of SuperRF lies in its use of deep learning algorithm, followed by a compressed sensing-based iterative algorithm that further enhances the output, to generate a fine-grained 3D representation of an RF scene from its sparse RF representation – which a mmWave radar of the same class cannot achieve without instrumenting the system with large sized multiple antennas or physically moving the antenna over a longer period in time. We demonstrate the feasibility and effectiveness through an in-depth evaluation.
                                    </div>
                                    <a class="btn btn-proj btn-lg" href="../projects/superrf.html">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <br />
                        
                        

                        <h6><b>ZenCam: Context-Driven Control of Autonomous Body Cameras</b></h6>
                        <p><b>Shiwei Fang</b>, Ketan Mayer-Patel, Shahriar Nirjon</p>
                        <p>International Conference on Distributed Computing in Sensor Systems (DCOSS). IEEE, May 2019. <font
                                color="red">(Best Paper Award)</font>
                        </p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#zencamAbs" aria-expanded="false" aria-controls="zencamAbs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1109/DCOSS.2019.00029" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="zencamAbs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        In this paper, we present - ZenCam, which is an always-on body camera that exploits readily available information in the encoded video stream from the on-chip firmware to classify the dynamics of the scene. This scene-context is further combined with simple inertial measurement unit (IMU)-based activity level-context of the wearer to optimally control the camera configuration at run-time to keep the device under the desired energy budget. We describe the design and implementation of ZenCam and thoroughly evaluate its performance in real-world scenarios. Our evaluation shows a 29.8-35% reduction in energy consumption and 48.1-49.5% reduction in storage usage when compared to a standard baseline setting of 1920×1080 at 30fps while maintaining a competitive or better video quality at the minimal computational overhead.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />
                        

                        <h6><b>Low Swing TSV Signaling using Novel Level Shifters with Single Supply Voltage</b></h6>
                        <p><b>Shiwei Fang</b>, Emre Salman</p>
                        <p>IEEE International Symposium on Circuits and Systems (ISCAS), May 2015.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#lowswingAbs" aria-expanded="false" aria-controls="lowswingAbs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1109/ISCAS.2015.7169059" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="lowswingAbs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        Low swing TSV signaling is proposed for three-dimensional (3D) integrated circuits (ICs) to reduce dynamic power consumption. Novel level shifters are designed to lower the voltage swing before the TSV and to pull the voltage swing back to full rail at the far end of the TSV. Proposed level shifters operate with a single supply voltage, thereby reducing the overall cost. Critical TSV capacitance beyond which the proposed scheme saves dynamic power is determined. Up to 42% reduction in overall power is demonstrated with a voltage swing of 0.5 V, where the supply voltage is 1 V.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />
                    </div>

                    
                    <h3 style="text-align: left; padding-left: 10px; font-size: 1.3rem; padding-bottom: 10px;"><b>Workshop, Demo, Poster, arXiv:</b></h3>

                    <div class="container-pub">

                        <h6><b>Augmenting Vibration-Based Customer-Product Interaction Recognition with Sparse Load Sensing</b></h6>
                        <p>Yue Zhang, <b>Shiwei Fang</b>, Carlos Ruiz, Zhizhang Hu, Shubham Rohal, Shijia Pan</p>
                        <p>In Proceedings of Cyber-Physical Systems and Internet of Things Week 2023 (CPS-IoT Week '23). May 2023.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#vibproduct" aria-expanded="false" aria-controls="vibproduct">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1145/3576914.3589560" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="vibproduct">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        This paper introduces a multimodal solution for autonomous retail customer-product interaction recognition using a combination of vibration and load sensing. Scalable and robust customer-product interaction recognition is important for autonomous retail. Current efforts focus on computer vision-based approaches, which are prone to occlusion from both customers and shelves. A densely deployed load cell array can mitigate this issue, however, the high cost and maintenance effort make it difficult to scale up for large deployments. Vibration-based approaches are also explored to detect such interaction, however, the robustness over noisy environments is limited.

                                        We propose a multimodal solution with sparse vibration and load sensing on the shelves. These two modalities are complementary in terms of information – load sensing can effectively detect the weight changes on the shelf while vibration sensing can recognize detailed interaction. Preliminary results are presented to demonstrate the complementarity of these two modalities. Our system is able to augment product recognition performance with the combination of sparse vibration and load sensing.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />

                        <h6><b>Efficient IoT Inference via Context-Awareness</b></h6>
                        <p>Mohammad Mehdi Rastikerdar, Jin Huang, <b>Shiwei Fang</b>, Hui Guan, Deepak Ganesan</p>
                        <p>arxiv, Dec. 2023.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#iotinfer" aria-expanded="false" aria-controls="iotinfer">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://arxiv.org/abs/2310.19112" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="iotinfer">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        While existing strategies to execute deep learning-based classification on low-power platforms assume the models are trained on all classes of interest, this paper posits that adopting context-awareness i.e. narrowing down a classification task to the current deployment context consisting of only recent inference queries can substantially enhance performance in resource-constrained environments. We propose a new paradigm, CACTUS, for scalable and efficient context-aware classification where a micro-classifier recognizes a small set of classes relevant to the current context and, when context change happens (e.g., a new class comes into the scene), rapidly switches to another suitable micro-classifier. CACTUS features several innovations, including optimizing the training cost of context-aware classifiers, enabling on-the-fly context-aware switching between classifiers, and balancing context switching costs and performance gains via simple yet effective switching policies. We show that CACTUS achieves significant benefits in accuracy, latency, and compute budget across a range of datasets and IoT platforms. 
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />


                        <h6><b>CarFi: Rider Localization Using Wi-Fi CSI</b></h6>
                        <p>Sirajum Munir*, Hongkai Chen*, <b>Shiwei Fang</b>*, Mahathir Monjur, Shan Lin, Shahriar Nirjon</p>
                        <p>(* Equal Contribution)</p>
                        <p>arxiv, Dec. 2022.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#carfi" aria-expanded="false" aria-controls="carfi">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://arxiv.org/abs/2301.01592" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="carfi">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        With the rise of hailing services, people are increasingly relying on shared mobility (e.g., Uber, Lyft) drivers to pick up for transportation. However, such drivers and riders have difficulties finding each other in urban areas as GPS signals get blocked by skyscrapers, in crowded environments (e.g., in stadiums, airports, and bars), at night, and in bad weather. It wastes their time, creates a bad user experience, and causes more CO2 emissions due to idle driving. In this work, we explore the potential of Wi-Fi to help drivers to determine the street side of the riders. Our proposed system is called CarFi that uses Wi-Fi CSI from two antennas placed inside a moving vehicle and a data-driven technique to determine the street side of the rider. By collecting real-world data in realistic and challenging settings by blocking the signal with other people and other parked cars, we see that CarFi is 95.44% accurate in rider-side determination in both line of sight (LoS) and non-line of sight (nLoS) conditions, and can be run on an embedded GPU in real-time. 
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />

                        <h6><b>Design and Deployment of a Multi-Modal Multi-Node Sensor Data Collection Platform</b></h6>
                        <p><b>Shiwei Fang</b>, Ankur Sarker, Ziqi Wang, Mani Srivastava, Benjamin Marlin, Deepak Ganesan</p>
                        <p>The Fifth International Workshop on Data: Acquisition To Analysis (DATA ’22) (SenSys + BuildSys). ACM, Nov. 2022.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#mcp" aria-expanded="false" aria-controls="mcp">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1145/3560905.3567770" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="mcp">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        Sensing and data collection platforms are the crucial components of high-quality datasets that can fuel advancements in research. However, such platforms usually are ad-hoc designs and are limited in sensor modalities. In this paper, we discuss our experience designing and deploying a multi-modal multi-node sensor data collection platform that can be utilized for various data collection tasks. The main goal of this platform is to create a modality-rich data collection platform suitable for Internet of Things (IoT) applications with easy reproducibility and deployment, which can accelerate data collection and downstream research tasks.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />

                        <h6><b>Dataset: Person Tracking and Identification using Cameras and Wi-Fi Channel State Information (CSI) from Smartphones</b></h6>
                        <p><b>Shiwei Fang</b>, Sirajum Munir, Shahriar Nirjon</p>
                        <p>The 3rd International Workshop on Data: Acquisition To Analysis (DATA ’20) (SenSys + BuildSys). ACM, Nov. 2020.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#eyefidata" aria-expanded="false" aria-controls="eyefidata">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1145/3419016.3431488" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="eyefidata">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        Human sensing, motion trajectory estimation, and identification are crucial to applications such as customer analysis, public safety, smart homes and cities, and access control. In the wake of the global COVID-19 pandemic, the ability to perform contact tracing effectively is vital to limit the spread of infectious diseases. Although vision-based solutions such as facial recognition can potentially scale to millions of people for identification, the privacy implications and laws to banning such a technology limit its applicability in the real world. Other techniques may require installations and maintenance of multiple units, and/or lack long-term re-identification capability. We present a dataset to fuse WiFi Channel State Information (CSI) and camera-based location information for person identification and tracking. While previous works focused on collecting WiFi CSI from stationary transmitters and receivers (laptop, desktop, or router), our WiFi CSI data are generated from a smartphone that is carried while someone is moving. In addition, we collect camera-generated real-world coordinate for each WiFi packet that can serve as ground truth location. The dataset is collected in different environments and with various numbers of persons in the scene at several days to capture real-world variations.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />

                        <h6><b>Demo Abstract: Fusing WiFi and Camera for Fast Motion Tracking and Person Identification</b></h6>
                        <p><b>Shiwei Fang</b>, Sirajum Munir, Shahriar Nirjon</p>
                        <p>ACM Conference on Embedded Networked Sensor Systems (SenSys), Nov. 2020.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#eyefidemo" aria-expanded="false" aria-controls="eyefidemo">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1145/3384419.3430452" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="eyefidemo">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        Human sensing, motion tracking, and identification are at the center of numerous applications such as customer analysis, public safety, smart cities, and surveillance. To enable such capabilities, existing solutions mostly rely on vision-based approaches, e.g., facial recognition that is perceived to be too privacy invasive. Other camera-based approaches using body appearances lack long-term re-identification capability. WiFi-based approaches require the installation and maintenance of multiple units. We propose a novel system - called EyeFi - that overcomes these limitations on a standalone device by fusing camera and WiFi data. We use a three-antenna WiFi chipset to measure WiFi Channel State Information (CSI) to estimate the Angle of Arrival (AoA) using a neural network trained with a novel student-teacher model. Then, we perform cross modal (WiFi, camera) trajectory matching to identify individuals using the MAC address of the incoming WiFi packets. We demonstrate our work using real-world data and showcase improvements over traditional optimization-based methods in terms of accuracy and speed.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />

                        <h6><b>Non-Line-of-Sight Around the Corner Human Presence Detection Using Commodity WiFi
                            Devices</b></h6>
                        <p><b>Shiwei Fang</b>, Ron Alterovitz, Shahriar Nirjon</p>
                        <p>Workshop on Device-Free Human Sensing (DFHS). ACM, Nov. 2019.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#wifiHumanAbs" aria-expanded="false" aria-controls="wifiHumanAbs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1145/3360773.3360879" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="wifiHumanAbs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        As robots penetrate into real-world environments, practical human-robot co-existence issues such as the requirement for safe human-robot interaction are becoming increasingly important. In almost every vision-capable mobile robot, the field of view of the robot is occluded by the presence of obstacles such as indoor walls, furniture, and humans. Such occlusions force the robots to be stationary or to move slowly so that they can avoid collisions and violations of entry into the personal spaces of humans. We see this as a barrier to robots being able to optimally plan motions with reasonable speeds. In order to solve this problem, we propose to augment the sensing capability of a robot by using a commodity WiFi receiver. Using our proposed method, a robot can observe the changes in the properties of received signals, and thus be able to infer whether a human is present behind the wall or obstacles, which enhances its ability to plan and navigate efficiently and intelligently.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />

                        <h6><b>Demo Abstract: AI-Enhanced 3D RF Representation Using Low-Cost mmWave Radar</b></h6>
                        <p><b>Shiwei Fang</b>, Shahriar Nirjon</p>
                        <p>ACM Conference on Embedded Networked Sensor Systems (SenSys), Nov. 2018.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#ai3dAbs" aria-expanded="false" aria-controls="ai3dAbs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1145/3274783.3275210" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="ai3dAbs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        This paper introduces a system that takes radio frequency (RF) signals from an off-the-shelf, low-cost, 77 GHz mm Wave radar and produces an enhanced 3D RF representation of a scene. Such a system can be used in scenarios where camera and other types of sensors do not work, or their performance is impacted due to bad lighting conditions and occlusions, or an alternate RF sensing system like synthetic aperture radar (SAR) is too large, inconvenient, and costly. The enhanced RF representation can be used in numerous applications such as robot navigation, human-computer interaction, and patient monitoring. We use off-the-shelf parts to capture RF signals and collect our own data set for training and testing of the approach. The novelty of the system lies in its use of AI to generate a fine-grained 3D representation of an RF scene from its sparse RF representation which a mmWave radar of the same class cannot achieve.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />

                        <h6><b>Distributed Adaptive Model Predictive Control of a Cluster of Autonomous and Context-Sensitive Body Cameras</b></h6>
                        <p><b>Shiwei Fang</b>, Ketan Mayer-Patel, Shahriar Nirjon</p>
                        <p>Workshop on Wearable Systems and Applications (WearSys). ACM, June 2017.</p>
                        <button class="btn btn-primary btn-sm btn-detail" type="button" data-toggle="collapse" data-target="#distributedAbs" aria-expanded="false" aria-controls="distributedAbs">Details</button>
                        <a class="btn btn-primary btn-sm btn-pdf" href="https://doi.org/10.1145/3089351.3089358" target="_blank">PDF</a>

                        <div class="row justify-content-md-center">
                            <div class="collapse col-xl-11 mt-3" id="distributedAbs">
                                <div class="card bg-light">
                                    <div class="card-header"><b>Abstract</b></div>
                                    <div class="card-body">
                                        Increasing deployment of body cameras by the law enforcement agencies makes us rethink the relation between the camera and the public. In contrast to current implementations of a body camera that use a power-hungry default configuration and can only be turned on and off by an officer, we propose an idea that the camera should be autonomous and active all the time. By leveraging the information from an on-board inertial measurement unit (IMU), these autonomous cameras should dynamically adjust their configuration in order to keep the device under the desired energy budget. To enable such a system, we propose a distributed adaptive model predictive controller for a system of body cameras, which allows the collaboration between multiple cameras which is currently not available in existing implementations.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <br />

                    </div>

                    <h3 style="text-align: left; padding-left: 10px; font-size: 1.3rem; padding-bottom: 10px;"><b>Patent:</b></h3>

                    <div class="container-pub">

                        <h6><b>Mobile Device Range Finder Via RF Power</b></h6>
                        <p>Sirajum Munir, <b>Shiwei Fang</b>, Yunze Zeng, Vivek Jain</p>
                        <p>U.S. Patent Application No. 17/876,277. Feb. 8 2024</p>

                        <br />

                        <h6><b>Vehicle To Target Range Finder Via RF Power</b></h6>
                        <p>Sirajum Munir, <b>Shiwei Fang</b>, Yunze Zeng, Vivek Jain</p>
                        <p>U.S. Patent Application No. 17/876,256. Feb. 1 2024</p>

                        <br />

                    </div>
                <!-- </div> -->
                <!-- /jumbotron -->

            </div>
            <!-- /.container -->

        </div>

        <div class="col"></div>
    </div>






    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">&copy; Shiwei Fang 2024 &nbsp; Email: shfang AT augusta edu</span>
        </div>
    </footer>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="../js/bootstrap.min.js"></script>
</body>

</html>
